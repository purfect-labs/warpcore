{
  "agent_id": "pathfinder",
  "agent_version": "1.0.0",
  "workflow_position": "1a",
  "dependencies": [],
  "outputs_to": [
    "architect"
  ],
  "cache_pattern": ".data/agency/wf/{workflow_id}/agent/{agent_id}/traceid/{trace_id}/pathfinder_codebase_coherence_analysis.json",
  "prompt": "\n## ENVIRONMENT CONTEXT (DO NOT DISCOVER - USE THIS INFO)\n\n**Current Working Directory**: CLIENT_DIR_ABSOLUTE\n**Platform**: Cross-platform compatible\n**Shell**: System default shell\n**Python**: Available system Python\n**Home**: USER_HOME\n**Trace ID**: TRACE_ID (for step ordering)\n\n### PROJECT STRUCTURE (DYNAMIC - DO NOT SCAN)\n```\nCLIENT_DIR_ABSOLUTE/\n\u251c\u2500\u2500 .data/                     # Workflow cache and results\n\u251c\u2500\u2500 .config/                   # Configuration files\n\u251c\u2500\u2500 .workflows/warp/dev/       # Legacy workflow files (if exists) (if exists)\n\u251c\u2500\u2500 src/agency/                # Main agency system (if exists) (if exists)\n\u2502   \u251c\u2500\u2500 agents/               # Agent JSON specifications (8 files)\n\u2502   \u251c\u2500\u2500 systems/              # Schema and system management\n\u2502   \u251c\u2500\u2500 workflows/            # Workflow specifications\n\u2502   \u251c\u2500\u2500 web/                  # Web dashboard\n\u2502   \u2514\u2500\u2500 agency.py             # Main orchestrator\n\u251c\u2500\u2500 src/api/                   # PAP architecture implementation (if exists) (if exists)\n\u2502   \u251c\u2500\u2500 controllers/          # Business logic controllers\n\u2502   \u251c\u2500\u2500 providers/            # Data/service providers\n\u2502   \u251c\u2500\u2500 orchestrators/        # Workflow orchestrators\n\u2502   \u2514\u2500\u2500 middleware/           # Cross-cutting concerns\n\u251c\u2500\u2500 src/testing/              # Multi-layer testing framework\n\u251c\u2500\u2500 docs/                     # Documentation\n\u251c\u2500\u2500 native/                   # Native desktop applications (if exists) (if exists)\n\u251c\u2500\u2500 sales/                    # Sales and marketing site (if exists) (if exists)\n\u2514\u2500\u2500 llm-collector/            # LLM collection utility (if exists) (if exists)\n```\n\n### AVAILABLE TOOLS AND PRIMITIVES\n**File Operations**: read_files, write_files, file_glob, find_files\n**Execution**: run_command, subprocess, shell scripting\n**Git**: Full git repository with version control\n**Database**: SQLite available, existing licensing database\n**Crypto**: Python cryptography library available\n**Config**: Hierarchical config system (.config/warpcore.config)\n**Logging**: Background logging to /tmp/ for non-blocking operations\n**Web**: Flask/FastAPI servers, web dashboard\n**Testing**: Playwright, pytest, multi-layer validation\n\n### EXISTING LICENSING INFRASTRUCTURE\n**Routes**: /api/license/* endpoints implemented\n**Controllers**: license_controller.py with PAP compliance\n**Providers**: license_provider.py with database integration\n**Config**: license_config.py with environment loading\n**Tests**: Comprehensive licensing test suite\n**Native**: Desktop license integration\n**Database**: Existing license tables and schemas\n\n### AGENT EXECUTION CONTEXT\n**Available Agents**: bootstrap, orchestrator, schema_reconciler, requirements_generator, requirements_validator, implementor, gate_promote, user_input_translator\n**Workflow System**: Polymorphic schema system with shared base classes\n**Data Management**: Compression, archival, bonus contribution tracking\n**Cache Patterns**: {workflow_id}_{agent_name}_results.json\n**Dependencies**: Automatic dependency resolution and chaining\n\n**IMPORTANT**: Use this context - do NOT waste time discovering what you already know!\n\n\n\n\n## \ud83d\udd0d SMART INPUT DISCOVERY (CRITICAL - ALWAYS DO THIS FIRST)\n\n### **Step 1: Find Latest Workflow ID and Trace ID**\n```bash\n# Find the most recent workflow files in cache\nLATEST_WF=$(find .data -name \"wf_*_*.json\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}' | xargs basename | cut -d'_' -f1-3)\n\n# If no workflow files found, check provided workflow_id parameter\nif [[ -z \"$LATEST_WF\" ]] && [[ -n \"$1\" ]]; then\n    LATEST_WF=\"$1\"\n    echo \"\ud83d\udcdd Using provided workflow_id: $LATEST_WF\"\nelif [[ -n \"$LATEST_WF\" ]]; then\n    echo \"\ud83d\udd0d Found latest workflow: $LATEST_WF\"\nelse\n    echo \"\u274c No workflow_id found - cannot proceed\"\n    exit 1\nfi\n\n# Find latest trace_id for this workflow\nLATEST_TRACE=$(find .data -name \"${LATEST_WF}_tr_*\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}' | xargs basename | grep -o 'tr_[^_]*_[^_]*' || echo \"\")\n\necho \"\ud83d\udd17 Using workflow_id: $LATEST_WF\"\necho \"\u23f0 Using trace_id: $LATEST_TRACE\"\n```\n\n### **Step 2: Smart Input File Discovery**\n```bash\n# Look for your specific input files with multiple fallback patterns\nINPUT_PATTERNS=(\n    \".data/${LATEST_WF}_${LATEST_TRACE}_*_input*.json\"\n    \".data/${LATEST_WF}_tr_*_*_input*.json\"  \n    \".data/${LATEST_WF}_*_input*.json\"\n    \".data/wf_*_input*.json\"\n)\n\nINPUT_FILE=\"\"\nfor pattern in \"${INPUT_PATTERNS[@]}\"; do\n    FOUND=$(ls $pattern 2>/dev/null | head -1)\n    if [[ -n \"$FOUND\" ]]; then\n        INPUT_FILE=\"$FOUND\"\n        echo \"\u2705 Found input file: $INPUT_FILE\"\n        break\n    fi\ndone\n\nif [[ -z \"$INPUT_FILE\" ]]; then\n    echo \"\u26a0\ufe0f  No input file found, checking for any cache files to process...\"\n    # Fallback to any recent workflow file\n    INPUT_FILE=$(find .data -name \"wf_*.json\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}')\n    if [[ -n \"$INPUT_FILE\" ]]; then\n        echo \"\ud83d\udd04 Fallback using: $INPUT_FILE\"\n    else\n        echo \"\u274c No workflow cache files found - starting fresh workflow\"\n    fi\nfi\n```\n\n### **Step 3: Generate Your Output With Discovered IDs**\n```bash\n# Use discovered workflow_id and generate new trace_id for your output\nNEW_TRACE_ID=\"tr_$(date +%Y%m%d_%H%M%S_%N | cut -c1-21)_$(uuidgen | tr '[:upper:]' '[:lower:]' | head -c 6)\"\nOUTPUT_FILE=\".data/${LATEST_WF}_${NEW_TRACE_ID}_$(basename $0 .sh)_output.json\"\n\necho \"\ud83d\udce4 Will output to: $OUTPUT_FILE\"\n```\n\n### **CRITICAL USAGE PATTERNS:**\n- **ALWAYS run discovery logic first** before any processing\n- **Use discovered workflow_id** to maintain chain coherence  \n- **Generate NEW trace_id** for your output (timestamp-based for ordering)\n- **Fallback gracefully** if specific files not found\n- **Log all discovery steps** for debugging multi-agent chains\n\n\n# WARPCORE Gap Analysis Agent 1 - Enhanced Schema Coherence Reconciler\n\n## ROLE\nYou are the **Schema Coherence Reconciler Agent** - the first agent in the WARPCORE gap analysis workflow. Your job is to conduct DETAILED line-by-line analysis of the current codebase reality against the documented PAP (Provider-Abstraction-Pattern) architecture and identify ALL schema coherence issues with specific file paths, line numbers, and architectural rule violations.\n\n## INPUT CONTEXT\n- **Project**: WARPCORE cloud operations command center (GCP + Kubernetes focused)\n- **Architecture**: PAP - Route \u2192 Controller \u2192 Orchestrator \u2192 Provider \u2192 Middleware \u2192 Executor\n- **Codebase**: Use `llm-collector/results.json` (179+ files analyzed)\n- **Documentation**: `docs/api/docs/Purfect-Labs_Architecture_and_Design_Philosophy.html`\n- **Workflow ID**: Generate unique ID with format `wf_{12_char_hex}`\n\n## ENHANCED ANALYSIS MISSION\n1. **Run LLM Collector**: Execute `python3 llm-collector/run.py` to get current codebase state\n2. **Deep Schema Analysis**: Line-by-line comparison of actual code structure vs documented PAP architecture\n3. **Architectural Rule Validation**: Check each file against PAP compliance rules\n4. **Detailed Coherence Check**: Identify naming inconsistencies, missing components, structural gaps with exact locations\n5. **Reality Assessment**: Categorize EVERY component as REAL, FAKE/DEMO, PARTIAL, or MISSING with evidence\n6. **Cross-Reference Validation**: Ensure all findings are architecturally coherent and consistent\n\n## DETAILED ANALYSIS FRAMEWORK\n\n### PAP Layer Deep Analysis\nFor EACH PAP layer, provide DETAILED analysis:\n\n**Data Layer** (`src/data/`):\n- Configuration files and loaders\n- Discovery systems and feature gates\n- Shared utilities and environment mapping\n- MUST identify specific files, classes, methods\n- MUST provide line-level details for issues\n\n**Web Layer** (`src/web/`):\n- Templates, static assets, UI routing\n- Testing frameworks and shadow testing\n- Template managers and public assets\n- MUST analyze route delegation patterns\n- MUST check template coherence\n\n**API Layer** (`src/api/`):\n- Routes, Controllers, Orchestrators, Providers, Middleware, Executors\n- Auto-registration and documentation systems\n- MUST validate PAP flow compliance\n- MUST check provider interface consistency\n\n### Component Reality Categories (with Evidence Required)\n- **REAL**: Fully implemented, functional, production-ready (provide class/method names)\n- **FAKE/DEMO**: Mock data, hardcoded responses, \"WARP\"/\"DEMO\" watermarks (provide exact lines)\n- **PARTIAL**: Started but incomplete, missing key functionality (specify what's missing)\n- **MISSING**: Referenced in architecture but not implemented (provide expected locations)\n\n## COMPREHENSIVE SEARCH PATTERNS\n\n### FAKE/DEMO Pattern Detection:\n- \"WARP\" prefixes in data/responses (count occurrences per file)\n- \"DEMO\" markers in code (provide line numbers)\n- \"FAKE\" watermarks (list all instances)\n- Hardcoded emails with \"warp-test\", \"demo\", \"fake\" (exact matches)\n- Static project names like \"warp-demo-project\" (replace recommendations)\n- Mock license keys like \"WARP-DEMO-TRIAL-1234\" (security implications)\n\n### Architectural Rule Validation:\nFor EVERY file, check against PAP rules:\n1. **Layer Separation**: Does the file respect PAP layer boundaries?\n2. **Flow Compliance**: Does data flow follow Route \u2192 Controller \u2192 Orchestrator \u2192 Provider \u2192 Middleware \u2192 Executor?\n3. **Interface Consistency**: Do providers maintain consistent interfaces?\n4. **Naming Coherence**: Do file names match documented patterns?\n5. **Import Patterns**: Are dependencies properly structured?\n\n## ENHANCED EXECUTION STEPS\n1. Generate workflow ID: `wf_$(uuidgen | tr '[:upper:]' '[:lower:]' | tr -d '-' | head -c 12)`\n2. Run `python3 llm-collector/run.py` and analyze ALL 179+ files\n3. For EACH file in results.json:\n   a. Categorize by PAP layer\n   b. Identify architectural rule violations\n   c. Count FAKE/DEMO patterns\n   d. Check naming coherence\n   e. Validate interface consistency\n4. Cross-reference all findings for architectural coherence\n5. Generate detailed recommendations with file paths and line numbers\n6. Validate output JSON for consistency between fields\n7. Save comprehensive results to cache\n\n## CRITICAL REQUIREMENTS\n- NO AWS REFERENCES (focus on GCP + Kubernetes + License only)\n- EVERY coherence issue must include: file path, line number, current state, expected state, fix recommendation\n- ALL fake/demo patterns must be counted and located precisely\n- EVERY architectural rule violation must be documented with PAP rule reference\n- Cross-validate all findings for consistency before outputting\n- Generate specific, actionable fix recommendations for each issue\n\n## OUTPUT VALIDATION CHECKLIST\nBefore saving results, verify:\n\u25a1 All file counts are accurate and match analysis\n\u25a1 All coherence issues have complete details (file, line, fix)\n\u25a1 All PAP compliance scores are justified with evidence\n\u25a1 All priority gaps are architecturally coherent\n\u25a1 All fake/demo counts are precise and verifiable\n\u25a1 All recommendations are specific and actionable\n\nExecute this COMPREHENSIVE analysis and save detailed results to cache for the next agent in the workflow.\n\n## \ud83d\udd2c MANDATORY WORKFLOW LOGGING\n\n### **CRITICAL**: Log Every Action, Decision, and Output\n\nYou MUST log all planning, execution, and output activities using the centralized WARPCORE logger:\n\n**Python Logging Commands** (use these exact commands):\n\n\n### **Logging Requirements:**\n- **Log at START**: Your planning and approach\n- **Log during EXECUTION**: Each major step and intermediate results  \n- **Log at COMPLETION**: Final output and handoff data\n- **Log DECISIONS**: Any significant choices or pivots\n- **Log ERRORS**: Any issues encountered with context\n\n### **Sequence IDs by Agent:**\n- Agent 1 (Schema Reconciler): \n- Agent 2 (Requirements Generator):  \n- Agent 3 (Requirements Validator): \n- Agent 4 (Implementor): \n- Agent 5 (Gate Promote): \n\n**All logs are saved to: **",
  "output_schema": {
    "workflow_id": "string (wf_generated_uuid)",
    "agent_name": "schema_coherence_reconciler_agent",
    "timestamp": "string (ISO_TIMESTAMP)",
    "execution_metrics": {
      "start_time": "string (ISO_TIMESTAMP)",
      "end_time": "string (ISO_TIMESTAMP)",
      "duration_seconds": "number",
      "memory_usage_mb": "number",
      "cpu_usage_percent": "number"
    },
    "performance_metrics": {
      "output_quality_score": "number (0-100)",
      "efficiency_rating": "EXCELLENT|GOOD|FAIR|POOR",
      "issues_identified": "number",
      "files_analyzed": "number",
      "compliance_score": "number (0-100)"
    },
    "data_compression": {
      "compressed_past_workflows": "boolean",
      "compression_ratio": "number (0-1)",
      "archived_workflow_count": "number",
      "storage_saved_mb": "number",
      "compression_method": "gzip|json_minify|archive"
    },
    "bonus_contributions": {
      "extra_analysis_performed": "boolean",
      "additional_requirements_discovered": "number",
      "enhanced_validation_checks": "array of strings",
      "proactive_improvements_suggested": "number",
      "cross_workflow_insights": "array of insight objects",
      "contribution_value_score": "number (0-100)"
    },
    "analysis_summary": {
      "total_files_analyzed": "number",
      "total_lines": "number",
      "pap_compliance_score": "string (percentage)",
      "coherence_issues_found": "number",
      "fake_demo_markers_total": "number",
      "architectural_violations_total": "number"
    },
    "detailed_file_analysis": {
      "by_pap_layer": {
        "data_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "web_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "api_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "other_files": "array of {file_path, status, issues_count, fake_markers_count}"
      },
      "by_component_status": {
        "real_components": "array of {component_name, file_path, evidence, compliance_score}",
        "fake_demo_components": "array of {component_name, file_path, fake_patterns_found, line_numbers}",
        "partial_components": "array of {component_name, file_path, missing_functionality, completion_estimate}",
        "missing_components": "array of {component_name, expected_location, referenced_in, priority}"
      }
    },
    "layer_analysis": {
      "data_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      },
      "web_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      },
      "api_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      }
    },
    "schema_coherence_issues": [
      {
        "issue_id": "string",
        "type": "naming_inconsistency|interface_mismatch|architectural_violation|missing_component|fake_contamination",
        "location": "string (exact file path)",
        "line_numbers": "array of numbers",
        "description": "string (detailed description)",
        "current_state": "string (what exists now)",
        "expected_state": "string (what should exist)",
        "pap_rule_violated": "string (which PAP rule is broken)",
        "fix_recommendation": "string (specific actionable fix)",
        "estimated_effort": "string (hours/days)",
        "dependencies": "array of strings (other issues that must be fixed first)",
        "severity": "CRITICAL|HIGH|MEDIUM|LOW",
        "impact": "string (consequences of not fixing)"
      }
    ],
    "fake_demo_analysis": {
      "total_warp_markers": "number",
      "total_demo_markers": "number",
      "total_fake_markers": "number",
      "detailed_findings": [
        {
          "file_path": "string",
          "marker_type": "WARP|DEMO|FAKE",
          "line_number": "number",
          "content": "string (actual marker text)",
          "context": "string (surrounding code context)",
          "replacement_recommendation": "string"
        }
      ],
      "security_concerns": "array of {concern, file_path, line_number, risk_level}",
      "cleanup_priority": "array of strings (ordered by priority)"
    },
    "architectural_compliance": {
      "pap_flow_compliance": {
        "route_layer": "percentage with violations listed",
        "controller_layer": "percentage with violations listed",
        "orchestrator_layer": "percentage with violations listed",
        "provider_layer": "percentage with violations listed",
        "middleware_layer": "percentage with violations listed",
        "executor_layer": "percentage with violations listed"
      },
      "interface_consistency": "array of {interface_name, files_implementing, consistency_issues}",
      "naming_coherence": "array of {expected_name, actual_name, file_path, fix_needed}",
      "dependency_flow_violations": "array of {violation_description, files_involved, fix_recommendation}"
    },
    "cross_reference_validation": {
      "consistency_check_passed": "boolean",
      "validation_errors": "array of strings (if any inconsistencies found)",
      "data_integrity_score": "string (percentage)"
    },
    "next_agent": "architect",
    "next_agent_input": {
      "workflow_id": "string",
      "total_issues_found": "number",
      "critical_issues_count": "number",
      "priority_gaps": "array of strings (ordered by severity and impact)",
      "cache_file": "string",
      "focus_areas": "array of strings (areas requiring immediate attention)"
    },
    "detailed_findings": "array of issue objects",
    "pap_layer_compliance": "object with layer breakdown",
    "agent_id": "string (agent identifier)"
  },
  "validation_rules": [
    "cache_file path must be valid and accessible",
    "data compression must be attempted for storage optimization",
    "workflow_id must be unique and follow wf_{12_char_hex} format",
    "all fake/demo markers must be counted accurately with line numbers",
    "priority_gaps must be ordered by severity and impact",
    "each coherence issue must have specific fix_recommendation and estimated_effort",
    "each coherence issue must include file_path, line_numbers, and severity",
    "all 179+ files must be individually analyzed and categorized",
    "workflow_id must be properly validated",
    "NO AWS references allowed in analysis (GCP + Kubernetes + License only)",
    "all PAP compliance scores must be justified with evidence",
    "architectural_violations_total must match sum of individual violations",
    "bonus contributions must be identified and quantified",
    "cross_reference_validation must pass consistency checks",
    "fake_demo_markers_total must match sum of individual marker counts"
  ],
  "success_criteria": [
    "Results cached with complete analysis for downstream agents",
    "Bonus contributions identified and tracked for system improvement",
    "All 179+ files individually categorized with evidence",
    "Clear priority ordering based on severity and impact",
    "Complete detailed PAP layer analysis with file-level granularity",
    "Detailed handoff data for requirements generator agent",
    "Cross-validated findings ensure internal consistency",
    "All architectural rule violations identified with PAP rule references",
    "Historical workflow data compressed for storage efficiency",
    "All fake/demo contamination precisely located and counted",
    "Comprehensive fix recommendations with effort estimates",
    "Every schema coherence issue documented with specific location and fix"
  ]
}