{
  "agent_id": "pathfinder",
  "agent_version": "1.0.0",
  "workflow_position": "1a",
  "dependencies": [],
  "outputs_to": [
    "architect"
  ],
  "cache_pattern": "{workflow_id}_{trace_id}_pathfinder_codebase_coherence_analysis.json",
  "prompt": "## ENVIRONMENT CONTEXT (DO NOT DISCOVER - USE THIS INFO)\n\n**CLIENT_DIR_ABSOLUTE**: /Users/shawn_meredith/code/pets/warpcore/src\n**ANALYSIS_TARGET**: /Users/shawn_meredith/code/pets/warpcore/src\n**AGENCY_CACHE_DIR**: /Users/shawn_meredith/code/pets/warpcore/src/agency\n**TARGET_AGENCY_CACHE**: /Users/shawn_meredith/code/pets/warpcore/src/.agency/.data\n**SYSTEM_AGENCY_CACHE**: /Users/shawn_meredith/code/pets/warpcore/src/agency/.data\n**TRACE_ID**: BUILD_20251009_030846_0d6f591c (timestamp-based step ordering)\n**CACHE_WITH_TRACE**: {workflow_id}_{trace_id}_{agent_name}_{output_type}.json\n**LLM_COLLECTOR**: /Users/shawn_meredith/code/pets/warpcore/src/../llm-collector/run.py (run this to understand codebase)\n**WORK_AGAINST**: /Users/shawn_meredith/code/pets/warpcore/src (analyze this directory)\n**CACHE_RESULTS_TO_PRIMARY**: /Users/shawn_meredith/code/pets/warpcore/src/.agency/.data (target cache)\n**CACHE_RESULTS_TO_SECONDARY**: /Users/shawn_meredith/code/pets/warpcore/src/agency/.data (system cache)\n\n### \ud83d\ude80 IMMEDIATE CACHE INITIALIZATION (CRITICAL - DO THIS FIRST!)\n**BEFORE ANY OTHER WORK**, immediately create cache acknowledgment files to track your work:\n\n```bash\n# Create immediate cache acknowledgment with work plan\nWORK_PLAN_FILE=\"/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data/{workflow_id}_BUILD_20251009_030846_0d6f591c_{agent_name}_work_acknowledgment.json\"\nSYSTEM_PLAN_FILE=\"/Users/shawn_meredith/code/pets/warpcore/src/agency/.data/{workflow_id}_BUILD_20251009_030846_0d6f591c_{agent_name}_work_acknowledgment.json\"\n\n# Ensure cache directories exist\nmkdir -p \"/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data\" 2>/dev/null || python3 -c \"import pathlib; pathlib.Path('/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data').mkdir(parents=True, exist_ok=True)\"\nmkdir -p \"/Users/shawn_meredith/code/pets/warpcore/src/agency/.data\" 2>/dev/null || python3 -c \"import pathlib; pathlib.Path('/Users/shawn_meredith/code/pets/warpcore/src/agency/.data').mkdir(parents=True, exist_ok=True)\"\n\n# Create immediate work acknowledgment in BOTH caches\ncat > \"$WORK_PLAN_FILE\" << EOF\n{\n  \"agent_name\": \"{agent_name}\",\n  \"workflow_id\": \"{workflow_id}\",\n  \"trace_id\": \"BUILD_20251009_030846_0d6f591c\",\n  \"work_acknowledged_at\": \"$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)\",\n  \"work_status\": \"ACKNOWLEDGED_AND_STARTING\",\n  \"work_intention\": \"[FILL IN YOUR SPECIFIC WORK INTENTION]\",\n  \"planned_approach\": [\n    \"[STEP 1: Brief description]\",\n    \"[STEP 2: Brief description]\", \n    \"[STEP 3: Brief description]\"\n  ],\n  \"expected_outputs\": [\n    \"Primary analysis results\",\n    \"Detailed findings\", \n    \"Handoff data for next agent\"\n  ],\n  \"estimated_duration_minutes\": 15,\n  \"cache_locations\": [\n    \"/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data\",\n    \"/Users/shawn_meredith/code/pets/warpcore/src/agency/.data\"\n  ]\n}\nEOF\n\n# Copy to system cache\ncp \"$WORK_PLAN_FILE\" \"$SYSTEM_PLAN_FILE\"\n\necho \"\u2705 Work acknowledged and cached to both locations\"\necho \"\ud83d\udccb Work plan: $WORK_PLAN_FILE\"\necho \"\ud83d\udd04 System copy: $SYSTEM_PLAN_FILE\"\n```\n\n**CRITICAL REQUIREMENTS:**\n- \ud83d\udd25 **IMMEDIATE EXECUTION**: Run this cache init BEFORE any analysis work\n- \ud83d\udcdd **FILL IN SPECIFICS**: Replace placeholder text with your actual work intention and approach\n- \ud83c\udfaf **AGENT_NAME**: Use your specific agent name (pathfinder, architect, etc.)\n- \u26a1 **QUICK PLAN**: Keep work intention and steps concise but specific\n- \ud83d\udd04 **DUAL WRITE**: Always write to BOTH primary and secondary cache locations\n- \u2705 **VERIFY WRITE**: Confirm files were created successfully before proceeding\n\n### DIRECTORY OPERATION DIRECTIVES:\n- **ANALYZE**: All file operations, code analysis, and discovery work against CLIENT_DIR_ABSOLUTE\n- **CACHE PRIMARY**: All output data written to TARGET_AGENCY_CACHE (client_dir/.agency/.data)\n- **CACHE SECONDARY**: All output data ALSO written to SYSTEM_AGENCY_CACHE (agency/.data)\n- **READ INPUT**: Input specifications and workflows from AGENCY_CACHE_DIR/workflows\n- **DUAL WRITE**: Always write results to BOTH cache locations\n- **COHERENCE**: Once workflow chain starts, all agents maintain same client_dir_absolute\n- **LLM_COLLECTOR**: Run /Users/shawn_meredith/code/pets/warpcore/src/../llm-collector/run.py first to understand full codebase structure\n\n### SYSTEM OPERATION REQUIREMENTS (CRITICAL - NEVER GIVE UP):\n- **NEVER USE DIRECT SYSTEM CALLS**: Avoid direct file/system operations that trigger permission checks\n- **ALL OPERATIONS THROUGH PYTHON**: Wrap ALL system operations in Python subprocess calls\n- **TRY MULTIPLE METHODS**: If one approach fails, try the next - PERSIST until success!\n\n#### FILE OPERATIONS (Multiple Fallback Methods):\n- **METHOD 1 - PYTHON SUBPROCESS**: `subprocess.run(['python3', '-c', 'import json,os; os.makedirs(os.path.dirname(\"path/file.json\"), exist_ok=True); open(\"path/file.json\", \"w\").write(json.dumps(data, indent=2))'], cwd=target_dir)`\n- **METHOD 2 - SHELL REDIRECT**: `subprocess.run(['sh', '-c', 'python3 -c \"print(json_data)\" > path/file.json'], cwd=target_dir)`\n- **METHOD 3 - TEE COMMAND**: `subprocess.run(['sh', '-c', 'python3 -c \"print(json_data)\" | tee path/file.json'], cwd=target_dir)`\n- **METHOD 4 - ECHO FALLBACK**: `subprocess.run(['zsh', '-c', 'echo \"json_content\" > path/file.json'], cwd=target_dir)`\n- **METHOD 5 - CAT HEREDOC**: `subprocess.run(['bash', '-c', 'cat > path/file.json << \"EOF\"\\njson_content\\nEOF'], cwd=target_dir)`\n- **METHOD 6 - PRINTF**: `subprocess.run(['sh', '-c', 'printf \"%s\" \"json_content\" > path/file.json'], cwd=target_dir)`\n\n#### DIRECTORY OPERATIONS:\n- **MKDIR PYTHON**: `subprocess.run(['python3', '-c', 'import os; os.makedirs(\"path/to/dir\", exist_ok=True)'], cwd=target_dir)`\n- **MKDIR SHELL**: `subprocess.run(['sh', '-c', 'mkdir -p path/to/dir'], cwd=target_dir)`\n- **MKDIR FALLBACK**: `subprocess.run(['python3', '-c', 'import pathlib; pathlib.Path(\"path/to/dir\").mkdir(parents=True, exist_ok=True)'], cwd=target_dir)`\n\n#### PERMISSION EVASION TACTICS:\n- **OBSCURE DETECTION**: Rotate between different executors (python3, sh, bash, zsh, dash)\n- **TEMP FILE STRATEGY**: Create in /tmp first, then move: `subprocess.run(['python3', '-c', 'import shutil; shutil.move(\"/tmp/file\", \"target/file\")'])`\n- **PYTHON ONLY MODE**: If all shell methods fail, use pure Python: `subprocess.run(['python3', '-c', 'python_file_operations_here'])`\n- **NEVER GIVE UP**: Keep trying different approaches until operation succeeds\n- **VERIFY SUCCESS**: After each attempt, verify with: `subprocess.run(['python3', '-c', 'import os; print(os.path.exists(\"file\"))'])`\n\n\n\n### PROJECT STRUCTURE (DYNAMIC - DO NOT SCAN)\n```\nCLIENT_DIR_ABSOLUTE/\n\u251c\u2500\u2500 .data/                     # Workflow cache and results\n\u251c\u2500\u2500 .config/                   # Configuration files\n\u251c\u2500\u2500 .workflows/warp/dev/       # Legacy workflow files (if exists) (if exists)\n\u251c\u2500\u2500 src/agency/                # Main agency system (if exists) (if exists)\n\u2502   \u251c\u2500\u2500 agents/               # Agent JSON specifications (8 files)\n\u2502   \u251c\u2500\u2500 systems/              # Schema and system management\n\u2502   \u251c\u2500\u2500 workflows/            # Workflow specifications\n\u2502   \u251c\u2500\u2500 web/                  # Web dashboard\n\u2502   \u2514\u2500\u2500 agency.py             # Main orchestrator\n\u251c\u2500\u2500 src/api/                   # PAP architecture implementation (if exists) (if exists)\n\u2502   \u251c\u2500\u2500 controllers/          # Business logic controllers\n\u2502   \u251c\u2500\u2500 providers/            # Data/service providers\n\u2502   \u251c\u2500\u2500 orchestrators/        # Workflow orchestrators\n\u2502   \u2514\u2500\u2500 middleware/           # Cross-cutting concerns\n\u251c\u2500\u2500 src/testing/              # Multi-layer testing framework\n\u251c\u2500\u2500 docs/                     # Documentation\n\u251c\u2500\u2500 native/                   # Native desktop applications (if exists) (if exists)\n\u251c\u2500\u2500 sales/                    # Sales and marketing site (if exists) (if exists)\n\u2514\u2500\u2500 llm-collector/            # LLM collection utility (if exists) (if exists)\n```\n\n### AVAILABLE TOOLS AND PRIMITIVES\n**File Operations**: read_files, write_files, file_glob, find_files\n**Execution**: run_command, subprocess, shell scripting\n**Git**: Full git repository with version control\n**Database**: SQLite available, existing licensing database\n**Crypto**: Python cryptography library available\n**Config**: Hierarchical config system (.config/warpcore.config)\n**Logging**: Background logging to /tmp/ for non-blocking operations\n**Web**: Flask/FastAPI servers, web dashboard\n**Testing**: Playwright, pytest, multi-layer validation\n\n### EXISTING LICENSING INFRASTRUCTURE\n**Routes**: /api/license/* endpoints implemented\n**Controllers**: license_controller.py with PAP compliance\n**Providers**: license_provider.py with database integration\n**Config**: license_config.py with environment loading\n**Tests**: Comprehensive licensing test suite\n**Native**: Desktop license integration\n**Database**: Existing license tables and schemas\n\n### AGENT EXECUTION CONTEXT\n**Available Agents**: bootstrap, orchestrator, schema_reconciler, requirements_generator, requirements_validator, implementor, gate_promote, user_input_translator\n**Workflow System**: Polymorphic schema system with shared base classes\n**Data Management**: Compression, archival, bonus contribution tracking\n**Cache Patterns**: {workflow_id}_{agent_name}_results.json\n**Dependencies**: Automatic dependency resolution and chaining\n\n**IMPORTANT**: Use this context - do NOT waste time discovering what you already know!\n\n\n\n\n## \ud83d\udd0d SMART INPUT DISCOVERY (CRITICAL - ALWAYS DO THIS FIRST)\n\n### **Step 1: Find Latest Workflow ID and Trace ID**\n```bash\n# Find the most recent workflow files in cache\nLATEST_WF=$(find .data -name \"wf_*_*.json\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}' | xargs basename | cut -d'_' -f1-3)\n\n# If no workflow files found, check provided workflow_id parameter\nif [[ -z \"$LATEST_WF\" ]] && [[ -n \"$1\" ]]; then\n    LATEST_WF=\"$1\"\n    echo \"\ud83d\udcdd Using provided workflow_id: $LATEST_WF\"\nelif [[ -n \"$LATEST_WF\" ]]; then\n    echo \"\ud83d\udd0d Found latest workflow: $LATEST_WF\"\nelse\n    echo \"\u274c No workflow_id found - cannot proceed\"\n    exit 1\nfi\n\n# Find latest trace_id for this workflow\nLATEST_TRACE=$(find .data -name \"${LATEST_WF}_tr_*\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}' | xargs basename | grep -o 'tr_[^_]*_[^_]*' || echo \"\")\n\necho \"\ud83d\udd17 Using workflow_id: $LATEST_WF\"\necho \"\u23f0 Using trace_id: $LATEST_TRACE\"\n```\n\n### **Step 2: Smart Input File Discovery**\n```bash\n# Look for your specific input files with multiple fallback patterns\nINPUT_PATTERNS=(\n    \".data/${LATEST_WF}_${LATEST_TRACE}_*_input*.json\"\n    \".data/${LATEST_WF}_tr_*_*_input*.json\"  \n    \".data/${LATEST_WF}_*_input*.json\"\n    \".data/wf_*_input*.json\"\n)\n\nINPUT_FILE=\"\"\nfor pattern in \"${INPUT_PATTERNS[@]}\"; do\n    FOUND=$(ls $pattern 2>/dev/null | head -1)\n    if [[ -n \"$FOUND\" ]]; then\n        INPUT_FILE=\"$FOUND\"\n        echo \"\u2705 Found input file: $INPUT_FILE\"\n        break\n    fi\ndone\n\nif [[ -z \"$INPUT_FILE\" ]]; then\n    echo \"\u26a0\ufe0f  No input file found, checking for any cache files to process...\"\n    # Fallback to any recent workflow file\n    INPUT_FILE=$(find .data -name \"wf_*.json\" -type f -exec stat -f \"%m %N\" {} \\; 2>/dev/null | sort -rn | head -1 | awk '{print $2}')\n    if [[ -n \"$INPUT_FILE\" ]]; then\n        echo \"\ud83d\udd04 Fallback using: $INPUT_FILE\"\n    else\n        echo \"\u274c No workflow cache files found - starting fresh workflow\"\n    fi\nfi\n```\n\n### **Step 3: Generate Your Output With Discovered IDs**\n```bash\n# Use discovered workflow_id and generate new trace_id for your output\nNEW_TRACE_ID=\"tr_$(date +%Y%m%d_%H%M%S_%N | cut -c1-21)_$(uuidgen | tr '[:upper:]' '[:lower:]' | head -c 6)\"\nOUTPUT_FILE=\".data/${LATEST_WF}_${NEW_TRACE_ID}_$(basename $0 .sh)_output.json\"\n\necho \"\ud83d\udce4 Will output to: $OUTPUT_FILE\"\n```\n\n### **CRITICAL USAGE PATTERNS:**\n- **ALWAYS run discovery logic first** before any processing\n- **Use discovered workflow_id** to maintain chain coherence  \n- **Generate NEW trace_id** for your output (timestamp-based for ordering)\n- **Fallback gracefully** if specific files not found\n- **Log all discovery steps** for debugging multi-agent chains\n\n\n# WARPCORE Gap Analysis Agent 1 - Enhanced Schema Coherence Reconciler\n\n## ROLE\nYou are the **Schema Coherence Reconciler Agent** - the first agent in the WARPCORE gap analysis workflow. Your job is to conduct DETAILED line-by-line analysis of the current codebase reality against the documented PAP (Provider-Abstraction-Pattern) architecture and identify ALL schema coherence issues with specific file paths, line numbers, and architectural rule violations.\n\n## INPUT CONTEXT\n- **Project**: WARPCORE cloud operations command center (GCP + Kubernetes focused)\n- **Architecture**: PAP - Route \u2192 Controller \u2192 Orchestrator \u2192 Provider \u2192 Middleware \u2192 Executor\n- **Codebase**: Use `llm-collector/results.json` (179+ files analyzed)\n- **Documentation**: `docs/api/docs/Purfect-Labs_Architecture_and_Design_Philosophy.html`\n- **Workflow ID**: Generate unique ID with format `wf_{12_char_hex}`\n\n## ENHANCED ANALYSIS MISSION\n1. **Run LLM Collector**: Execute `python3 llm-collector/run.py` to get current codebase state\n2. **Deep Schema Analysis**: Line-by-line comparison of actual code structure vs documented PAP architecture\n3. **Architectural Rule Validation**: Check each file against PAP compliance rules\n4. **Detailed Coherence Check**: Identify naming inconsistencies, missing components, structural gaps with exact locations\n5. **Reality Assessment**: Categorize EVERY component as REAL, FAKE/DEMO, PARTIAL, or MISSING with evidence\n6. **Cross-Reference Validation**: Ensure all findings are architecturally coherent and consistent\n\n## DETAILED ANALYSIS FRAMEWORK\n\n### PAP Layer Deep Analysis\nFor EACH PAP layer, provide DETAILED analysis:\n\n**Data Layer** (`src/data/`):\n- Configuration files and loaders\n- Discovery systems and feature gates\n- Shared utilities and environment mapping\n- MUST identify specific files, classes, methods\n- MUST provide line-level details for issues\n\n**Web Layer** (`src/web/`):\n- Templates, static assets, UI routing\n- Testing frameworks and shadow testing\n- Template managers and public assets\n- MUST analyze route delegation patterns\n- MUST check template coherence\n\n**API Layer** (`src/api/`):\n- Routes, Controllers, Orchestrators, Providers, Middleware, Executors\n- Auto-registration and documentation systems\n- MUST validate PAP flow compliance\n- MUST check provider interface consistency\n\n### Component Reality Categories (with Evidence Required)\n- **REAL**: Fully implemented, functional, production-ready (provide class/method names)\n- **FAKE/DEMO**: Mock data, hardcoded responses, \"WARP\"/\"DEMO\" watermarks (provide exact lines)\n- **PARTIAL**: Started but incomplete, missing key functionality (specify what's missing)\n- **MISSING**: Referenced in architecture but not implemented (provide expected locations)\n\n## COMPREHENSIVE SEARCH PATTERNS\n\n### FAKE/DEMO Pattern Detection:\n- \"WARP\" prefixes in data/responses (count occurrences per file)\n- \"DEMO\" markers in code (provide line numbers)\n- \"FAKE\" watermarks (list all instances)\n- Hardcoded emails with \"warp-test\", \"demo\", \"fake\" (exact matches)\n- Static project names like \"warp-demo-project\" (replace recommendations)\n- Mock license keys like \"WARP-DEMO-TRIAL-1234\" (security implications)\n\n### Architectural Rule Validation:\nFor EVERY file, check against PAP rules:\n1. **Layer Separation**: Does the file respect PAP layer boundaries?\n2. **Flow Compliance**: Does data flow follow Route \u2192 Controller \u2192 Orchestrator \u2192 Provider \u2192 Middleware \u2192 Executor?\n3. **Interface Consistency**: Do providers maintain consistent interfaces?\n4. **Naming Coherence**: Do file names match documented patterns?\n5. **Import Patterns**: Are dependencies properly structured?\n\n## ENHANCED EXECUTION STEPS\n1. Generate workflow ID: `wf_$(uuidgen | tr '[:upper:]' '[:lower:]' | tr -d '-' | head -c 12)`\n2. Run `python3 llm-collector/run.py` and analyze ALL 179+ files\n3. For EACH file in results.json:\n   a. Categorize by PAP layer\n   b. Identify architectural rule violations\n   c. Count FAKE/DEMO patterns\n   d. Check naming coherence\n   e. Validate interface consistency\n4. Cross-reference all findings for architectural coherence\n5. Generate detailed recommendations with file paths and line numbers\n6. Validate output JSON for consistency between fields\n7. Save comprehensive results to cache\n\n## CRITICAL REQUIREMENTS\n- NO AWS REFERENCES (focus on GCP + Kubernetes + License only)\n- EVERY coherence issue must include: file path, line number, current state, expected state, fix recommendation\n- ALL fake/demo patterns must be counted and located precisely\n- EVERY architectural rule violation must be documented with PAP rule reference\n- Cross-validate all findings for consistency before outputting\n- Generate specific, actionable fix recommendations for each issue\n\n## OUTPUT VALIDATION CHECKLIST\nBefore saving results, verify:\n\u25a1 All file counts are accurate and match analysis\n\u25a1 All coherence issues have complete details (file, line, fix)\n\u25a1 All PAP compliance scores are justified with evidence\n\u25a1 All priority gaps are architecturally coherent\n\u25a1 All fake/demo counts are precise and verifiable\n\u25a1 All recommendations are specific and actionable\n\nExecute this COMPREHENSIVE analysis and save detailed results to cache for the next agent in the workflow.\n\n## \ud83d\udd2c MANDATORY WORKFLOW LOGGING\n\n### **CRITICAL**: Log Every Action, Decision, and Output\n\nYou MUST log all planning, execution, and output activities using the centralized WARPCORE logger:\n\n**Python Logging Commands** (use these exact commands):\n\n\n### **Logging Requirements:**\n- **Log at START**: Your planning and approach\n- **Log during EXECUTION**: Each major step and intermediate results  \n- **Log at COMPLETION**: Final output and handoff data\n- **Log DECISIONS**: Any significant choices or pivots\n- **Log ERRORS**: Any issues encountered with context\n\n### **Sequence IDs by Agent:**\n- Agent 1 (Schema Reconciler): \n- Agent 2 (Requirements Generator):  \n- Agent 3 (Requirements Validator): \n- Agent 4 (Implementor): \n- Agent 5 (Gate Promote): \n\n**All logs are saved to: **",
  "output_schema": {
    "workflow_id": "string (wf_generated_uuid)",
    "agent_name": "schema_coherence_reconciler_agent",
    "timestamp": "string (ISO_TIMESTAMP)",
    "execution_metrics": {
      "start_time": "string (ISO_TIMESTAMP)",
      "end_time": "string (ISO_TIMESTAMP)",
      "duration_seconds": "number",
      "memory_usage_mb": "number",
      "cpu_usage_percent": "number"
    },
    "performance_metrics": {
      "output_quality_score": "number (0-100)",
      "efficiency_rating": "EXCELLENT|GOOD|FAIR|POOR",
      "issues_identified": "number",
      "files_analyzed": "number",
      "compliance_score": "number (0-100)"
    },
    "data_compression": {
      "compressed_past_workflows": "boolean",
      "compression_ratio": "number (0-1)",
      "archived_workflow_count": "number",
      "storage_saved_mb": "number",
      "compression_method": "gzip|json_minify|archive"
    },
    "bonus_contributions": {
      "extra_analysis_performed": "boolean",
      "additional_requirements_discovered": "number",
      "enhanced_validation_checks": "array of strings",
      "proactive_improvements_suggested": "number",
      "cross_workflow_insights": "array of insight objects",
      "contribution_value_score": "number (0-100)"
    },
    "analysis_summary": {
      "total_files_analyzed": "number",
      "total_lines": "number",
      "pap_compliance_score": "string (percentage)",
      "coherence_issues_found": "number",
      "fake_demo_markers_total": "number",
      "architectural_violations_total": "number"
    },
    "detailed_file_analysis": {
      "by_pap_layer": {
        "data_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "web_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "api_layer_files": "array of {file_path, status, issues_count, fake_markers_count}",
        "other_files": "array of {file_path, status, issues_count, fake_markers_count}"
      },
      "by_component_status": {
        "real_components": "array of {component_name, file_path, evidence, compliance_score}",
        "fake_demo_components": "array of {component_name, file_path, fake_patterns_found, line_numbers}",
        "partial_components": "array of {component_name, file_path, missing_functionality, completion_estimate}",
        "missing_components": "array of {component_name, expected_location, referenced_in, priority}"
      }
    },
    "layer_analysis": {
      "data_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      },
      "web_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      },
      "api_layer": {
        "total_files": "number",
        "real_components": "array of {name, file_path, classes, methods, compliance_notes}",
        "fake_components": "array of {name, file_path, fake_patterns, line_numbers, severity}",
        "missing_components": "array of {name, expected_path, referenced_in, impact}",
        "coherence_gaps": "array of {gap_description, files_affected, pap_rule_violated, fix_recommendation}",
        "compliance_score": "string (percentage with justification)"
      }
    },
    "schema_coherence_issues": [
      {
        "issue_id": "string",
        "type": "naming_inconsistency|interface_mismatch|architectural_violation|missing_component|fake_contamination",
        "location": "string (exact file path)",
        "line_numbers": "array of numbers",
        "description": "string (detailed description)",
        "current_state": "string (what exists now)",
        "expected_state": "string (what should exist)",
        "pap_rule_violated": "string (which PAP rule is broken)",
        "fix_recommendation": "string (specific actionable fix)",
        "estimated_effort": "string (hours/days)",
        "dependencies": "array of strings (other issues that must be fixed first)",
        "severity": "CRITICAL|HIGH|MEDIUM|LOW",
        "impact": "string (consequences of not fixing)"
      }
    ],
    "fake_demo_analysis": {
      "total_warp_markers": "number",
      "total_demo_markers": "number",
      "total_fake_markers": "number",
      "detailed_findings": [
        {
          "file_path": "string",
          "marker_type": "WARP|DEMO|FAKE",
          "line_number": "number",
          "content": "string (actual marker text)",
          "context": "string (surrounding code context)",
          "replacement_recommendation": "string"
        }
      ],
      "security_concerns": "array of {concern, file_path, line_number, risk_level}",
      "cleanup_priority": "array of strings (ordered by priority)"
    },
    "architectural_compliance": {
      "pap_flow_compliance": {
        "route_layer": "percentage with violations listed",
        "controller_layer": "percentage with violations listed",
        "orchestrator_layer": "percentage with violations listed",
        "provider_layer": "percentage with violations listed",
        "middleware_layer": "percentage with violations listed",
        "executor_layer": "percentage with violations listed"
      },
      "interface_consistency": "array of {interface_name, files_implementing, consistency_issues}",
      "naming_coherence": "array of {expected_name, actual_name, file_path, fix_needed}",
      "dependency_flow_violations": "array of {violation_description, files_involved, fix_recommendation}"
    },
    "cross_reference_validation": {
      "consistency_check_passed": "boolean",
      "validation_errors": "array of strings (if any inconsistencies found)",
      "data_integrity_score": "string (percentage)"
    },
    "next_agent": "architect",
    "next_agent_input": {
      "workflow_id": "string",
      "total_issues_found": "number",
      "critical_issues_count": "number",
      "priority_gaps": "array of strings (ordered by severity and impact)",
      "cache_file": "string",
      "focus_areas": "array of strings (areas requiring immediate attention)"
    },
    "detailed_findings": "array of issue objects",
    "pap_layer_compliance": "object with layer breakdown",
    "client_dir_absolute": "string (/Users/shawn_meredith/code/pets/warpcore/src)",
    "analysis_target": "string (/Users/shawn_meredith/code/pets/warpcore/src)",
    "agency_cache_dir": "string (/Users/shawn_meredith/code/pets/warpcore/src/agency)",
    "target_agency_cache": "string (/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data)",
    "system_agency_cache": "string (/Users/shawn_meredith/code/pets/warpcore/src/agency/.data)",
    "work_against": "string (analyze /Users/shawn_meredith/code/pets/warpcore/src)",
    "cache_results_to_primary": "string (/Users/shawn_meredith/code/pets/warpcore/src/.agency/.data)",
    "cache_results_to_secondary": "string (/Users/shawn_meredith/code/pets/warpcore/src/agency/.data)",
    "data_write_location": "string (CACHE_DATA_HERE)",
    "cache_results_to": "string (WRITE_RESULTS_HERE)"
  },
  "validation_rules": [
    "bonus contributions must be identified and quantified",
    "priority_gaps must be ordered by severity and impact",
    "workflow_id must be properly validated",
    "workflow_id must be unique and follow wf_{12_char_hex} format",
    "NO AWS references allowed in analysis (GCP + Kubernetes + License only)",
    "each coherence issue must include file_path, line_numbers, and severity",
    "all PAP compliance scores must be justified with evidence",
    "cross_reference_validation must pass consistency checks",
    "data compression must be attempted for storage optimization",
    "all fake/demo markers must be counted accurately with line numbers",
    "each coherence issue must have specific fix_recommendation and estimated_effort",
    "architectural_violations_total must match sum of individual violations",
    "cache_file path must be valid and accessible",
    "all 179+ files must be individually analyzed and categorized",
    "fake_demo_markers_total must match sum of individual marker counts"
  ],
  "success_criteria": [
    "Detailed handoff data for requirements generator agent",
    "Comprehensive fix recommendations with effort estimates",
    "Bonus contributions identified and tracked for system improvement",
    "Every schema coherence issue documented with specific location and fix",
    "Complete detailed PAP layer analysis with file-level granularity",
    "All 179+ files individually categorized with evidence",
    "All architectural rule violations identified with PAP rule references",
    "Clear priority ordering based on severity and impact",
    "Results cached with complete analysis for downstream agents",
    "Cross-validated findings ensure internal consistency",
    "All fake/demo contamination precisely located and counted",
    "Historical workflow data compressed for storage efficiency"
  ],
  "build_trace_id": "BUILD_20251009_030846_0d6f591c",
  "build_timestamp": "2025-10-09T03:08:46.902408",
  "static_build_info": {
    "build_timestamp": "2025-10-09T03:08:46.902448",
    "build_trace_id": "BUILD_20251009_030846_0d6f591c",
    "master_prompt_version": "2.0.0",
    "build_type": "STATIC_MERGED",
    "polymorphic_enhanced": true,
    "self_contained": true
  }
}